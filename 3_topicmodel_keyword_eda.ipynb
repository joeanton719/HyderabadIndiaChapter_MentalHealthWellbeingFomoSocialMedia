{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For better visualizations\n",
    "sns.set_theme(style=\"ticks\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 15\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "      <th>body_type</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1b0m6c1</td>\n",
       "      <td>r/CasualConversation is looking for new modera...</td>\n",
       "      <td>2024-02-26 16:48:14</td>\n",
       "      <td>Hello everyone,It's that time again, [we're lo...</td>\n",
       "      <td>post</td>\n",
       "      <td>22_subs_subreddits_trolls_subreddit</td>\n",
       "      <td>hello everyone time look new mod join team doc...</td>\n",
       "      <td>modmail courteous professional,grow strict quo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.592145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b5h6x7</td>\n",
       "      <td>Deleting social media was one of the best thin...</td>\n",
       "      <td>2024-03-03 13:29:12</td>\n",
       "      <td>I know that technically reddit is a social med...</td>\n",
       "      <td>post</td>\n",
       "      <td>1_fb_deleted_instagram_facebook</td>\n",
       "      <td>know technically reddit social medium not nega...</td>\n",
       "      <td>consider delete social,instagram tiktok,health...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.817773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1b5lu9j</td>\n",
       "      <td>Lonlieness is not about gender! My Opinion.</td>\n",
       "      <td>2024-03-03 16:56:09</td>\n",
       "      <td>Okay so I just wanted to get this off of my mi...</td>\n",
       "      <td>post</td>\n",
       "      <td>359_loneliness_lonely_epidemic_intiate</td>\n",
       "      <td>okay want get mind keep see people post man lo...</td>\n",
       "      <td>man loneliness epidemic,understand gender thin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.736416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1b5jo0m</td>\n",
       "      <td>Have you gave up any hobbies?</td>\n",
       "      <td>2024-03-03 15:23:08</td>\n",
       "      <td>I used to be a doll collector. Then when I was...</td>\n",
       "      <td>post</td>\n",
       "      <td>-1_breakfast_cream_listening_bus</td>\n",
       "      <td>use doll collector tell weird old donate every...</td>\n",
       "      <td>use doll collector,throw away hobby,day happy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.638780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1b5c8ah</td>\n",
       "      <td>Is it me or has the world just stopped moving ...</td>\n",
       "      <td>2024-03-03 08:24:06</td>\n",
       "      <td>This is strange, but I feel like:1) I don’t re...</td>\n",
       "      <td>post</td>\n",
       "      <td>-1_breakfast_cream_listening_bus</td>\n",
       "      <td>strange feel like not remember anything happen...</td>\n",
       "      <td>good year covid,like remember happen,people tr...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.756849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title  \\\n",
       "0  1b0m6c1  r/CasualConversation is looking for new modera...   \n",
       "1  1b5h6x7  Deleting social media was one of the best thin...   \n",
       "2  1b5lu9j       Lonlieness is not about gender! My Opinion.    \n",
       "3  1b5jo0m                      Have you gave up any hobbies?   \n",
       "4  1b5c8ah  Is it me or has the world just stopped moving ...   \n",
       "\n",
       "             timestamp                                               body  \\\n",
       "0  2024-02-26 16:48:14  Hello everyone,It's that time again, [we're lo...   \n",
       "1  2024-03-03 13:29:12  I know that technically reddit is a social med...   \n",
       "2  2024-03-03 16:56:09  Okay so I just wanted to get this off of my mi...   \n",
       "3  2024-03-03 15:23:08  I used to be a doll collector. Then when I was...   \n",
       "4  2024-03-03 08:24:06  This is strange, but I feel like:1) I don’t re...   \n",
       "\n",
       "  body_type                              topic_name  \\\n",
       "0      post     22_subs_subreddits_trolls_subreddit   \n",
       "1      post         1_fb_deleted_instagram_facebook   \n",
       "2      post  359_loneliness_lonely_epidemic_intiate   \n",
       "3      post        -1_breakfast_cream_listening_bus   \n",
       "4      post        -1_breakfast_cream_listening_bus   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  hello everyone time look new mod join team doc...   \n",
       "1  know technically reddit social medium not nega...   \n",
       "2  okay want get mind keep see people post man lo...   \n",
       "3  use doll collector tell weird old donate every...   \n",
       "4  strange feel like not remember anything happen...   \n",
       "\n",
       "                                            keywords sentiment_label  \\\n",
       "0  modmail courteous professional,grow strict quo...        positive   \n",
       "1  consider delete social,instagram tiktok,health...        positive   \n",
       "2  man loneliness epidemic,understand gender thin...        negative   \n",
       "3  use doll collector,throw away hobby,day happy ...        negative   \n",
       "4  good year covid,like remember happen,people tr...        negative   \n",
       "\n",
       "   sentiment_score  \n",
       "0         0.592145  \n",
       "1         0.817773  \n",
       "2         0.736416  \n",
       "3         0.638780  \n",
       "4         0.756849  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=\"datasets/reddit_posts_with_topics_keywords_sentiments.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_name\n",
       "-1_breakfast_cream_listening_bus        52156\n",
       "0_attacks_prescribed_symptoms_benzos     2555\n",
       "1_fb_deleted_instagram_facebook          1490\n",
       "2_nicotine_cigarette_smoked_smoker        953\n",
       "3_bday_birthdays_25th_belated             940\n",
       "4_masks_vaccinated_mask_wearing           867\n",
       "5_cats_cat_kitty_kitties                  740\n",
       "6_pete_stories_casconvo_sharing           648\n",
       "7_removed_backspaced_bailing_erased       572\n",
       "8_grammar_native_english_language         565\n",
       "9_dreamt_dreaming_dreamed_dreams          554\n",
       "10_crying_tear_cry_tears                  515\n",
       "11_bully_bullied_bullies_bullying         496\n",
       "12_upvote_upvotes_upvoted_downvotes       481\n",
       "13_bachelors_bachelor_diploma_ged         481\n",
       "14_channel_subreddit_sub_lego             469\n",
       "15_teens_30s_20s_aging                    461\n",
       "16_savings_invest_401k_saving             444\n",
       "17_pregnancy_adoption_pregnant_adopt      412\n",
       "18_bald_shaved_shave_shaving              405\n",
       "19_dads_fathers_father_dadding            399\n",
       "20_cloudy_gloomy_rain_storm               387\n",
       "21_showers_shower_baths_bath              386\n",
       "22_subs_subreddits_trolls_subreddit       366\n",
       "23_vet_pup_dog_dogs                       364\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df['topic_name'].value_counts()\n",
    "    .nlargest(25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'  # Default to noun\n",
    "\n",
    "def lemmatize_string(s):\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Split the string into words\n",
    "    words = s.split(\"_\")\n",
    "\n",
    "    # Get the POS tag for each word\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for w, pos in pos_tags]\n",
    "\n",
    "    # lemmatized_words = [lemmatizer.lemmatize(w, pos='v') for w in words]\n",
    "\n",
    "    # Remove duplicates while preserving order\n",
    "    lemmatized_words = list(dict.fromkeys(lemmatized_words))\n",
    "\n",
    "    # Join the words back into a string\n",
    "    result = \"_\".join(lemmatized_words)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatize function to topics name and then return as a dict\n",
    "lemmatized_topics_dict = (\n",
    "    df.loc[:, 'topic_name']\n",
    "    .drop_duplicates(ignore_index=True)\n",
    "    .to_frame('topics')\n",
    "    .assign(topic_num=lambda x: x['topics'].str.extract(r'^(-?\\d+)_', expand=False),\n",
    "            topic_name=lambda x: x['topics'].str.extract(r'^-?\\d+_(.+)$', expand=False))\n",
    "    .assign(lema_topic_name=lambda x: x['topic_num'] + \"_\"+ x['topic_name'].apply(lemmatize_string))\n",
    "    .set_index(\"topics\")\n",
    "    .loc[:, 'lema_topic_name']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Showing first 3 key-pairs only\n",
    "dict(list(lemmatized_topics_dict.items())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.assign(topics=lambda x: x['topic_name'].map(lemmatized_topics_dict))\n",
    "    .drop('topic_name', axis=1)\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df['topics']\n",
    "    .value_counts()\n",
    "    .nlargest(25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=5\n",
    "\n",
    "text_data = (\n",
    "    df.loc[lambda x: x['topics'].str.contains(fr'^{i}_'), 'keywords']\n",
    "    .replace(\" \", \"_\", regex=True)\n",
    "    .replace(\",\", \" \", regex=True)\n",
    "    .str.cat(sep=\" \")\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(tight_layout=True)\n",
    "\n",
    "wordcloud = WordCloud(width=1200,\n",
    "                      height=800,\n",
    "                      colormap='YlGnBu', \n",
    "                      background_color=\"black\").generate(text_data)\n",
    "\n",
    "ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax.axis(\"off\")\n",
    "ax.grid(None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
