{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x18e308dc1d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# NLTK packages\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# downloading corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove 'not' from stopword list\n",
    "stopwords.remove('not')\n",
    "\n",
    "# Load the spaCy English model\n",
    "# https://github.com/explosion/spaCy/issues/6498\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", 'ner'])\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>downs</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>comments</th>\n",
       "      <th>body_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1b3xroj</td>\n",
       "      <td>r/CasualConversation Welcome Thread - Month of...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>2024-03-01 16:00:09</td>\n",
       "      <td>Welcome to r/CasualConversation! Thank you for...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[{'comment_id': 'kt1kilo', 'author': 'fluffyke...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b0m6c1</td>\n",
       "      <td>r/CasualConversation is looking for new modera...</td>\n",
       "      <td>MajorParadox</td>\n",
       "      <td>2024-02-26 16:48:14</td>\n",
       "      <td>Hello everyone,\\n\\nIt's that time again, [we'r...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1b5h6x7</td>\n",
       "      <td>Deleting social media was one of the best thin...</td>\n",
       "      <td>Glum-Net-9774</td>\n",
       "      <td>2024-03-03 13:29:12</td>\n",
       "      <td>I know that technically reddit is a social med...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>[{'comment_id': 'kt5a08d', 'author': 'BaclavaB...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1b5lu9j</td>\n",
       "      <td>Lonlieness is not about gender! My Opinion.</td>\n",
       "      <td>Intjfire</td>\n",
       "      <td>2024-03-03 16:56:09</td>\n",
       "      <td>Okay so I just wanted to get this off of my mi...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>[{'comment_id': 'kt6d4uc', 'author': 'theTeaEn...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1b5jo0m</td>\n",
       "      <td>Have you gave up any hobbies?</td>\n",
       "      <td>incombusty</td>\n",
       "      <td>2024-03-03 15:23:08</td>\n",
       "      <td>I used to be a doll collector. Then when I was...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>[{'comment_id': 'kt67rvd', 'author': 'SA_Dza',...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title         author  \\\n",
       "0  1b3xroj  r/CasualConversation Welcome Thread - Month of...  AutoModerator   \n",
       "1  1b0m6c1  r/CasualConversation is looking for new modera...   MajorParadox   \n",
       "2  1b5h6x7  Deleting social media was one of the best thin...  Glum-Net-9774   \n",
       "3  1b5lu9j       Lonlieness is not about gender! My Opinion.        Intjfire   \n",
       "4  1b5jo0m                      Have you gave up any hobbies?     incombusty   \n",
       "\n",
       "            timestamp                                               body  \\\n",
       "0 2024-03-01 16:00:09  Welcome to r/CasualConversation! Thank you for...   \n",
       "1 2024-02-26 16:48:14  Hello everyone,\\n\\nIt's that time again, [we'r...   \n",
       "2 2024-03-03 13:29:12  I know that technically reddit is a social med...   \n",
       "3 2024-03-03 16:56:09  Okay so I just wanted to get this off of my mi...   \n",
       "4 2024-03-03 15:23:08  I used to be a doll collector. Then when I was...   \n",
       "\n",
       "   score  downs  total_comments  \\\n",
       "0    4.0    0.0             7.0   \n",
       "1    4.0    0.0             0.0   \n",
       "2   69.0    0.0            13.0   \n",
       "3   28.0    0.0            22.0   \n",
       "4   33.0    0.0            76.0   \n",
       "\n",
       "                                            comments body_type  \n",
       "0  [{'comment_id': 'kt1kilo', 'author': 'fluffyke...      post  \n",
       "1                                                 []      post  \n",
       "2  [{'comment_id': 'kt5a08d', 'author': 'BaclavaB...      post  \n",
       "3  [{'comment_id': 'kt6d4uc', 'author': 'theTeaEn...      post  \n",
       "4  [{'comment_id': 'kt67rvd', 'author': 'SA_Dza',...      post  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_datasets = [ds for ds in os.listdir(\"datasets\") if not ds.endswith(\".csv\")]\n",
    "\n",
    "df = (\n",
    "    pd.concat((pd.read_json(\"datasets/\"+d) for d in json_datasets), ignore_index=True)\n",
    "    .replace('', np.nan)\n",
    "    .loc[lambda x: x['posts'].isnull()]\n",
    "    .rename(columns={'posts' : 'body_type'})\n",
    "    .assign(body_type='post')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2289 entries, 0 to 2288\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   post_id         2289 non-null   object        \n",
      " 1   title           2289 non-null   object        \n",
      " 2   author          2211 non-null   object        \n",
      " 3   timestamp       2289 non-null   datetime64[ns]\n",
      " 4   body            2248 non-null   object        \n",
      " 5   score           2289 non-null   float64       \n",
      " 6   downs           2289 non-null   float64       \n",
      " 7   total_comments  2289 non-null   float64       \n",
      " 8   comments        2289 non-null   object        \n",
      " 9   body_type       2289 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(3), object(6)\n",
      "memory usage: 179.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dataset contains columns named `comments` and `replies`, both of which have nested JSON data. To make this data more accessible, we‚Äôll preprocess the dataset. This involves extracting the `body` from both the `comments` and `replies` and adding these to the dataset. Each `body` will be associated with its corresponding `post_id`, which helps us identify the post that each comment or reply belongs to. Additionally, we‚Äôll introduce a new column named `body_type`. This column will specify whether a particular body is a post, a comment, or a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unix_to_datetime(x: object) -> datetime:\n",
    "    \"\"\"\n",
    "    function to convert unix time format (of object datatype) to datetime format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return datetime.datetime.fromtimestamp(int(x))\n",
    "    except ValueError:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_json_cols(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    function that converts comments or replies column to a \n",
    "    Pandas DataFrame with reference to its original post_id\n",
    "    \"\"\"\n",
    "    \n",
    "    return (\n",
    "        df.loc[:, ['post_id', col]] # Select post_id and column (either `comments` or `replies`)\n",
    "        \n",
    "        # convert nested dictionary to its own rows with dictonary object\n",
    "        .explode(column=col, ignore_index=True) \n",
    "        \n",
    "        # remove those rows where the column has null values\n",
    "        # some posts have no comments and some comments have no replies.\n",
    "        .loc[lambda x: x[col].notnull()] \n",
    "\n",
    "        .reset_index(drop=True) \n",
    "        .pipe(lambda x: x.join(pd.json_normalize(x[col]))) # convert dictionary object to a dataframe and  \n",
    "                                                           # join with its original datafrmae to get\n",
    "                                                           # respective post_id\n",
    "        .drop(col, axis=1)\n",
    "        .assign(timestamp=lambda x: x['timestamp'].apply(convert_unix_to_datetime), # convert unix time fomat to datetime\n",
    "                body_type=col)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "      <th>body_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1b0m6c1</td>\n",
       "      <td>r/CasualConversation is looking for new modera...</td>\n",
       "      <td>2024-02-26 16:48:14</td>\n",
       "      <td>Hello everyone,It's that time again, [we're lo...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b5h6x7</td>\n",
       "      <td>Deleting social media was one of the best thin...</td>\n",
       "      <td>2024-03-03 13:29:12</td>\n",
       "      <td>I know that technically reddit is a social med...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1b5lu9j</td>\n",
       "      <td>Lonlieness is not about gender! My Opinion.</td>\n",
       "      <td>2024-03-03 16:56:09</td>\n",
       "      <td>Okay so I just wanted to get this off of my mi...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1b5jo0m</td>\n",
       "      <td>Have you gave up any hobbies?</td>\n",
       "      <td>2024-03-03 15:23:08</td>\n",
       "      <td>I used to be a doll collector. Then when I was...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1b5c8ah</td>\n",
       "      <td>Is it me or has the world just stopped moving ...</td>\n",
       "      <td>2024-03-03 08:24:06</td>\n",
       "      <td>This is strange, but I feel like:1) I don‚Äôt re...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title  \\\n",
       "0  1b0m6c1  r/CasualConversation is looking for new modera...   \n",
       "1  1b5h6x7  Deleting social media was one of the best thin...   \n",
       "2  1b5lu9j       Lonlieness is not about gender! My Opinion.    \n",
       "3  1b5jo0m                      Have you gave up any hobbies?   \n",
       "4  1b5c8ah  Is it me or has the world just stopped moving ...   \n",
       "\n",
       "             timestamp                                               body  \\\n",
       "0  2024-02-26 16:48:14  Hello everyone,It's that time again, [we're lo...   \n",
       "1  2024-03-03 13:29:12  I know that technically reddit is a social med...   \n",
       "2  2024-03-03 16:56:09  Okay so I just wanted to get this off of my mi...   \n",
       "3  2024-03-03 15:23:08  I used to be a doll collector. Then when I was...   \n",
       "4  2024-03-03 08:24:06  This is strange, but I feel like:1) I don‚Äôt re...   \n",
       "\n",
       "  body_type  \n",
       "0      post  \n",
       "1      post  \n",
       "2      post  \n",
       "3      post  \n",
       "4      post  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = (\n",
    "    # Unnest `comments` column and then concat with the whole dataframe\n",
    "    df.pipe(lambda x: pd.concat([x, expand_json_cols(x, 'comments')], ignore_index=True))\n",
    "\n",
    "    # Similarly, Unnest `replies` column and then concat with the whole dataframe\n",
    "    .pipe(lambda x: pd.concat([x, expand_json_cols(x, 'replies')], ignore_index=True))\n",
    "\n",
    "    # Dropping rows where comments/replied were deleted\n",
    "    # Also removing rows with posts by AutoModerator\n",
    "    .loc[lambda x: ~x['body'].eq(\"[deleted]\")]\n",
    "    .loc[lambda x: x['author'].ne('AutoModerator')]\n",
    "    .drop(['author', 'score', 'downs', 'total_comments', 'comments', \n",
    "           'comment_id', 'replies', 'reply_id'], axis=1)\n",
    "           \n",
    "    # replace reddit channel or comment links in the body \n",
    "    .replace(r'\\r+|\\n+|\\t+','', regex=True)\n",
    "    .assign(body=lambda x: (x['body']\n",
    "                            .replace(r'(r/.*?/comments/.*?) ', 'comments ', regex=True)\n",
    "                            .replace(r'(r/.*?) ', 'channel ', regex=True)))\n",
    "\n",
    "    # some body is empty. So these will be replaced with the title.\n",
    "    .assign(body=lambda x: np.where(x['body'].isnull(), x['title'], x['body']))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "      <th>body_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [post_id, title, timestamp, body, body_type]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensuring no null values\n",
    "full_df.loc[lambda x: x['body'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 133038 entries, 0 to 133037\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   post_id    133038 non-null  object\n",
      " 1   title      2286 non-null    object\n",
      " 2   timestamp  133038 non-null  object\n",
      " 3   body       133038 non-null  object\n",
      " 4   body_type  133038 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body_type\n",
       "comments    112412\n",
       "replies      18340\n",
       "post          2286\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['body_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if any reddit tags/links still exists in the body\n",
    "(\n",
    "    full_df['body'].str\n",
    "    .extract(r'(r/.*?/comments/.*?) ', expand=False)\n",
    "    .unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    full_df['body'].str\n",
    "    .extract(r'(r/.*?) ', expand=False)\n",
    "    .unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "      <th>body_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>1b3xroj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-03-02 22:37:27</td>\n",
       "      <td>hi   ..been here a couple of days.   great pla...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>1b3xroj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-03-02 22:39:22</td>\n",
       "      <td>I don't have much friends. I'm friendly with a...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id title            timestamp  \\\n",
       "2286  1b3xroj   NaN  2024-03-02 22:37:27   \n",
       "2287  1b3xroj   NaN  2024-03-02 22:39:22   \n",
       "\n",
       "                                                   body body_type  \n",
       "2286  hi   ..been here a couple of days.   great pla...  comments  \n",
       "2287  I don't have much friends. I'm friendly with a...  comments  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking all body that belings to post_id '1b3xroj'\n",
    "(\n",
    "    full_df.query(\"post_id=='1b3xroj'\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "      <th>body_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>Today I promised myself bacon and eggs if I co...</td>\n",
       "      <td>2019-03-15 01:55:17</td>\n",
       "      <td>Today I promised myself bacon and eggs if I co...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114679</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:17:18</td>\n",
       "      <td>Good for you! Whenever I see something with an...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114680</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:18:22</td>\n",
       "      <td>Dig the shirt</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114681</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:48:41</td>\n",
       "      <td>Hope the meet up went well! Proud to see you b...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114682</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 08:37:28</td>\n",
       "      <td>As a woman who suffers from anxiety, I really ...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114683</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:47:09</td>\n",
       "      <td>Treating yourself to something wonderful for o...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114684</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 07:17:54</td>\n",
       "      <td>awww you are so awesome!!!! too bad those baco...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114685</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:18:40</td>\n",
       "      <td>Good shit.</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114686</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 08:49:13</td>\n",
       "      <td>Your smile is so cute! Proud of you!!</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114687</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:24:09</td>\n",
       "      <td>Way to go! üòä that's wonderful.</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114688</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 07:35:39</td>\n",
       "      <td>That‚Äôs super awesome you attended this week, y...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114689</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 09:02:28</td>\n",
       "      <td>What a beautiful genuine smile!! I love the co...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114690</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 13:18:36</td>\n",
       "      <td>This sub is awesome. edit: My hands are up and...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114691</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-11-22 08:33:34</td>\n",
       "      <td>Nice tits</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114692</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 10:21:19</td>\n",
       "      <td>Fry the bacon longer, it doesn‚Äôt look crispy</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114693</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 07:11:51</td>\n",
       "      <td>That's a good idea.</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114694</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 07:36:29</td>\n",
       "      <td>Seeing your smile made me smile! Congratulatio...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114695</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 08:04:06</td>\n",
       "      <td>The meal looks delicious and you look great!</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114696</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 11:25:27</td>\n",
       "      <td>Your smile made me smile. Well done.</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114697</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 12:36:19</td>\n",
       "      <td>Boom! Congrats! That‚Äôs a big achievement.</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133019</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:37:22</td>\n",
       "      <td>Thanks. I really wanted a photo to share my mo...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133020</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:32:22</td>\n",
       "      <td>Thanks it‚Äôs Fasha‚Äôs top from Dragonball Z. I f...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133021</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 07:01:06</td>\n",
       "      <td>Thanks it did. I often feel like I don‚Äôt have ...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133022</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 08:40:22</td>\n",
       "      <td>Yeah f*** anxiety! üòÅ it helps me to remember t...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133023</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:55:29</td>\n",
       "      <td>I was being all tricksy with my monkey brain. ...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133024</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 07:27:19</td>\n",
       "      <td>Lol not something I can eat anymore. Ah the da...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133025</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:33:29</td>\n",
       "      <td>Thanks. Felt damn good to follow through!</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133026</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 08:50:42</td>\n",
       "      <td>Thanks. I often feel like I smile too much in ...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133027</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 07:07:31</td>\n",
       "      <td>üòÅ man this emoji is how I smile. So much teeth!</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133028</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 06:31:10</td>\n",
       "      <td>Yeah I‚Äôm so please I made it. Ended up having ...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133029</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 07:43:30</td>\n",
       "      <td>Yeah isn‚Äôt it! I seriously want to wear more a...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133030</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 09:04:08</td>\n",
       "      <td>Thanks, it‚Äôs what happens to my face when some...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133031</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 13:31:37</td>\n",
       "      <td>The issue with that move is the amount of time...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133032</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 11:19:35</td>\n",
       "      <td>Are you American by any chance? Americans seem...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133033</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 07:23:24</td>\n",
       "      <td>Thanks. It worked really well. It just gave me...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133034</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 07:44:39</td>\n",
       "      <td>It‚Äôs a full on smile. That was me trying to to...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133035</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 08:05:02</td>\n",
       "      <td>It was. üòÅ and thanks. üòä</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133036</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 11:26:05</td>\n",
       "      <td>Yay! Big smiles are infectious!</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133037</th>\n",
       "      <td>b18zmk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-15 12:40:43</td>\n",
       "      <td>Thanks! I think so too. Anytime I do something...</td>\n",
       "      <td>replies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id                                              title  \\\n",
       "2285    b18zmk  Today I promised myself bacon and eggs if I co...   \n",
       "114679  b18zmk                                                NaN   \n",
       "114680  b18zmk                                                NaN   \n",
       "114681  b18zmk                                                NaN   \n",
       "114682  b18zmk                                                NaN   \n",
       "114683  b18zmk                                                NaN   \n",
       "114684  b18zmk                                                NaN   \n",
       "114685  b18zmk                                                NaN   \n",
       "114686  b18zmk                                                NaN   \n",
       "114687  b18zmk                                                NaN   \n",
       "114688  b18zmk                                                NaN   \n",
       "114689  b18zmk                                                NaN   \n",
       "114690  b18zmk                                                NaN   \n",
       "114691  b18zmk                                                NaN   \n",
       "114692  b18zmk                                                NaN   \n",
       "114693  b18zmk                                                NaN   \n",
       "114694  b18zmk                                                NaN   \n",
       "114695  b18zmk                                                NaN   \n",
       "114696  b18zmk                                                NaN   \n",
       "114697  b18zmk                                                NaN   \n",
       "133019  b18zmk                                                NaN   \n",
       "133020  b18zmk                                                NaN   \n",
       "133021  b18zmk                                                NaN   \n",
       "133022  b18zmk                                                NaN   \n",
       "133023  b18zmk                                                NaN   \n",
       "133024  b18zmk                                                NaN   \n",
       "133025  b18zmk                                                NaN   \n",
       "133026  b18zmk                                                NaN   \n",
       "133027  b18zmk                                                NaN   \n",
       "133028  b18zmk                                                NaN   \n",
       "133029  b18zmk                                                NaN   \n",
       "133030  b18zmk                                                NaN   \n",
       "133031  b18zmk                                                NaN   \n",
       "133032  b18zmk                                                NaN   \n",
       "133033  b18zmk                                                NaN   \n",
       "133034  b18zmk                                                NaN   \n",
       "133035  b18zmk                                                NaN   \n",
       "133036  b18zmk                                                NaN   \n",
       "133037  b18zmk                                                NaN   \n",
       "\n",
       "                  timestamp  \\\n",
       "2285    2019-03-15 01:55:17   \n",
       "114679  2019-03-15 06:17:18   \n",
       "114680  2019-03-15 06:18:22   \n",
       "114681  2019-03-15 06:48:41   \n",
       "114682  2019-03-15 08:37:28   \n",
       "114683  2019-03-15 06:47:09   \n",
       "114684  2019-03-15 07:17:54   \n",
       "114685  2019-03-15 06:18:40   \n",
       "114686  2019-03-15 08:49:13   \n",
       "114687  2019-03-15 06:24:09   \n",
       "114688  2019-03-15 07:35:39   \n",
       "114689  2019-03-15 09:02:28   \n",
       "114690  2019-03-15 13:18:36   \n",
       "114691  2021-11-22 08:33:34   \n",
       "114692  2019-03-15 10:21:19   \n",
       "114693  2019-03-15 07:11:51   \n",
       "114694  2019-03-15 07:36:29   \n",
       "114695  2019-03-15 08:04:06   \n",
       "114696  2019-03-15 11:25:27   \n",
       "114697  2019-03-15 12:36:19   \n",
       "133019  2019-03-15 06:37:22   \n",
       "133020  2019-03-15 06:32:22   \n",
       "133021  2019-03-15 07:01:06   \n",
       "133022  2019-03-15 08:40:22   \n",
       "133023  2019-03-15 06:55:29   \n",
       "133024  2019-03-15 07:27:19   \n",
       "133025  2019-03-15 06:33:29   \n",
       "133026  2019-03-15 08:50:42   \n",
       "133027  2019-03-15 07:07:31   \n",
       "133028  2019-03-15 06:31:10   \n",
       "133029  2019-03-15 07:43:30   \n",
       "133030  2019-03-15 09:04:08   \n",
       "133031  2019-03-15 13:31:37   \n",
       "133032  2019-03-15 11:19:35   \n",
       "133033  2019-03-15 07:23:24   \n",
       "133034  2019-03-15 07:44:39   \n",
       "133035  2019-03-15 08:05:02   \n",
       "133036  2019-03-15 11:26:05   \n",
       "133037  2019-03-15 12:40:43   \n",
       "\n",
       "                                                     body body_type  \n",
       "2285    Today I promised myself bacon and eggs if I co...      post  \n",
       "114679  Good for you! Whenever I see something with an...  comments  \n",
       "114680                                      Dig the shirt  comments  \n",
       "114681  Hope the meet up went well! Proud to see you b...  comments  \n",
       "114682  As a woman who suffers from anxiety, I really ...  comments  \n",
       "114683  Treating yourself to something wonderful for o...  comments  \n",
       "114684  awww you are so awesome!!!! too bad those baco...  comments  \n",
       "114685                                         Good shit.  comments  \n",
       "114686              Your smile is so cute! Proud of you!!  comments  \n",
       "114687                     Way to go! üòä that's wonderful.  comments  \n",
       "114688  That‚Äôs super awesome you attended this week, y...  comments  \n",
       "114689  What a beautiful genuine smile!! I love the co...  comments  \n",
       "114690  This sub is awesome. edit: My hands are up and...  comments  \n",
       "114691                                          Nice tits  comments  \n",
       "114692      Fry the bacon longer, it doesn‚Äôt look crispy   comments  \n",
       "114693                               That's a good idea.   comments  \n",
       "114694  Seeing your smile made me smile! Congratulatio...  comments  \n",
       "114695       The meal looks delicious and you look great!  comments  \n",
       "114696              Your smile made me smile. Well done.   comments  \n",
       "114697         Boom! Congrats! That‚Äôs a big achievement.   comments  \n",
       "133019  Thanks. I really wanted a photo to share my mo...   replies  \n",
       "133020  Thanks it‚Äôs Fasha‚Äôs top from Dragonball Z. I f...   replies  \n",
       "133021  Thanks it did. I often feel like I don‚Äôt have ...   replies  \n",
       "133022  Yeah f*** anxiety! üòÅ it helps me to remember t...   replies  \n",
       "133023  I was being all tricksy with my monkey brain. ...   replies  \n",
       "133024  Lol not something I can eat anymore. Ah the da...   replies  \n",
       "133025          Thanks. Felt damn good to follow through!   replies  \n",
       "133026  Thanks. I often feel like I smile too much in ...   replies  \n",
       "133027    üòÅ man this emoji is how I smile. So much teeth!   replies  \n",
       "133028  Yeah I‚Äôm so please I made it. Ended up having ...   replies  \n",
       "133029  Yeah isn‚Äôt it! I seriously want to wear more a...   replies  \n",
       "133030  Thanks, it‚Äôs what happens to my face when some...   replies  \n",
       "133031  The issue with that move is the amount of time...   replies  \n",
       "133032  Are you American by any chance? Americans seem...   replies  \n",
       "133033  Thanks. It worked really well. It just gave me...   replies  \n",
       "133034  It‚Äôs a full on smile. That was me trying to to...   replies  \n",
       "133035                            It was. üòÅ and thanks. üòä   replies  \n",
       "133036                    Yay! Big smiles are infectious!   replies  \n",
       "133037  Thanks! I think so too. Anytime I do something...   replies  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking all body that belings to post_id 'b18zmk'\n",
    "(\n",
    "    full_df.query(\"post_id=='b18zmk'\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The above clearly shows the structure of the conversation for a `post_id`. There will be a single post, followed by comments and replies to those comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Topic modeling** is a technique used to automatically identify topics present in a text object by using unsupervised machine learning to find clusters or groups of similar words within a body of text. We will use **BERTopic** for topic modelling.\n",
    "\n",
    "##### **[BERTopic](\"https://maartengr.github.io/BERTopic/index.html\")** is an advanced technique in language topic modeling that leverages transformer models for superior language capabilities. It uses several components to produce topics from a collection of documents:\n",
    "\n",
    "* **Transformer Embedding Model**: This is used to convert words into meaningful numerical representations that capture the semantic meaning of the words in their context.\n",
    "\n",
    "* **UMAP Dimensionality Reduction**: This is a technique used to reduce the dimensionality of the word embeddings, making them easier to work with.\n",
    "\n",
    "* **HDBSCAN Clustering**: This is a clustering algorithm used to group similar word embeddings together. Each cluster represents a potential topic.\n",
    "\n",
    "* **Cluster Tagging using c-TF-IDF**: This is used to find the most representative terms for each topic, which are then used as the labels for the topics.\n",
    "\n",
    "BERTopic takes advantage of these components to create a model that can identify and extract topics from a collection of documents. It‚Äôs a powerful tool for analyzing and understanding large collections of unstructured text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note**: Performing Topic Modelling with BERTopic on a dataset of over 130,000 texts took approximately 45 minutes. After running the topic modelling for the first time in this notebook, I saved the results as a CSV file on my local system. This step ensures that I won‚Äôt need to repeat the entire process in future sessions. Please note that the sections of this notebook related to topic modelling are commented out. This precaution is taken in case a restart of the notebook is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertopic import BERTopic\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "# sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, \n",
    "#                                      reduce_frequent_words=True)\n",
    "\n",
    "# topic_model = BERTopic(embedding_model=sentence_model,\n",
    "#                        ctfidf_model=ctfidf_model,\n",
    "#                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = full_df['body']\n",
    "# topics, _ = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.visualize_topics(width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df_with_topics = (\n",
    "#     full_df.assign(topics=topics)\n",
    "#     .merge(topic_model.get_topic_info()[['Topic', 'Name']], \n",
    "#              how='left',\n",
    "#              left_on='topics',\n",
    "#              right_on='Topic')\n",
    "#     .drop(['Topic', 'topics'], axis=1)\n",
    "#     .rename(columns={'Name': 'topic_name'})\n",
    "# )\n",
    "\n",
    "# full_df_with_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df_with_topics.to_csv(\"datasets/reddit_posts_with_topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "      <th>body_type</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1b0m6c1</td>\n",
       "      <td>r/CasualConversation is looking for new modera...</td>\n",
       "      <td>2024-02-26 16:48:14</td>\n",
       "      <td>Hello everyone,It's that time again, [we're lo...</td>\n",
       "      <td>post</td>\n",
       "      <td>22_subs_subreddits_trolls_subreddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b5h6x7</td>\n",
       "      <td>Deleting social media was one of the best thin...</td>\n",
       "      <td>2024-03-03 13:29:12</td>\n",
       "      <td>I know that technically reddit is a social med...</td>\n",
       "      <td>post</td>\n",
       "      <td>1_fb_deleted_instagram_facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1b5lu9j</td>\n",
       "      <td>Lonlieness is not about gender! My Opinion.</td>\n",
       "      <td>2024-03-03 16:56:09</td>\n",
       "      <td>Okay so I just wanted to get this off of my mi...</td>\n",
       "      <td>post</td>\n",
       "      <td>359_loneliness_lonely_epidemic_intiate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1b5jo0m</td>\n",
       "      <td>Have you gave up any hobbies?</td>\n",
       "      <td>2024-03-03 15:23:08</td>\n",
       "      <td>I used to be a doll collector. Then when I was...</td>\n",
       "      <td>post</td>\n",
       "      <td>-1_breakfast_cream_listening_bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1b5c8ah</td>\n",
       "      <td>Is it me or has the world just stopped moving ...</td>\n",
       "      <td>2024-03-03 08:24:06</td>\n",
       "      <td>This is strange, but I feel like:1) I don‚Äôt re...</td>\n",
       "      <td>post</td>\n",
       "      <td>-1_breakfast_cream_listening_bus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title  \\\n",
       "0  1b0m6c1  r/CasualConversation is looking for new modera...   \n",
       "1  1b5h6x7  Deleting social media was one of the best thin...   \n",
       "2  1b5lu9j       Lonlieness is not about gender! My Opinion.    \n",
       "3  1b5jo0m                      Have you gave up any hobbies?   \n",
       "4  1b5c8ah  Is it me or has the world just stopped moving ...   \n",
       "\n",
       "             timestamp                                               body  \\\n",
       "0  2024-02-26 16:48:14  Hello everyone,It's that time again, [we're lo...   \n",
       "1  2024-03-03 13:29:12  I know that technically reddit is a social med...   \n",
       "2  2024-03-03 16:56:09  Okay so I just wanted to get this off of my mi...   \n",
       "3  2024-03-03 15:23:08  I used to be a doll collector. Then when I was...   \n",
       "4  2024-03-03 08:24:06  This is strange, but I feel like:1) I don‚Äôt re...   \n",
       "\n",
       "  body_type                              topic_name  \n",
       "0      post     22_subs_subreddits_trolls_subreddit  \n",
       "1      post         1_fb_deleted_instagram_facebook  \n",
       "2      post  359_loneliness_lonely_epidemic_intiate  \n",
       "3      post        -1_breakfast_cream_listening_bus  \n",
       "4      post        -1_breakfast_cream_listening_bus  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open saved df with topics\n",
    "topic_df = pd.read_csv(\"datasets/reddit_posts_with_topics.csv\")\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_name\n",
       "-1_breakfast_cream_listening_bus        52368\n",
       "0_attacks_prescribed_symptoms_benzos     2555\n",
       "1_fb_deleted_instagram_facebook          1490\n",
       "2_nicotine_cigarette_smoked_smoker        953\n",
       "3_bday_birthdays_25th_belated             940\n",
       "4_masks_vaccinated_mask_wearing           867\n",
       "5_cats_cat_kitty_kitties                  740\n",
       "6_pete_stories_casconvo_sharing           648\n",
       "7_removed_backspaced_bailing_erased       572\n",
       "8_grammar_native_english_language         565\n",
       "9_dreamt_dreaming_dreamed_dreams          554\n",
       "10_crying_tear_cry_tears                  515\n",
       "11_bully_bullied_bullies_bullying         496\n",
       "12_upvote_upvotes_upvoted_downvotes       481\n",
       "13_bachelors_bachelor_diploma_ged         481\n",
       "14_channel_subreddit_sub_lego             469\n",
       "15_teens_30s_20s_aging                    465\n",
       "16_savings_invest_401k_saving             444\n",
       "17_pregnancy_adoption_pregnant_adopt      412\n",
       "18_bald_shaved_shave_shaving              405\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 20 topics\n",
    "# -1 topics are outliers and should be ignored.\n",
    "(\n",
    "    topic_df['topic_name']\n",
    "    .value_counts()\n",
    "    .nlargest(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing & Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this section, the texts will be preprocessed such as stopword removal, lemmatization, etc following by keyword extraction on the cleaned text using **keyBERT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kootenpv/contractions\n",
    "import contractions\n",
    "\n",
    "# Remove stopwords function\n",
    "def remove_sw(text: str) -> str:\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    # Remove stopwords and those words that is of length<=2\n",
    "    processed_text = (word for word in tokenized_text if not word in stopwords and len(word) > 2)\n",
    "\n",
    "    return \" \".join(processed_text)\n",
    "\n",
    "# Lemmatization\n",
    "# https://stackoverflow.com/a/75215495/15937542\n",
    "def lemmatize_pipe(text_col: pd.Series) -> list:\n",
    "    \n",
    "    docs = nlp.pipe(text_col)\n",
    "    lemmatized_col = [lemmatize_text_spacy(doc) for doc in docs]\n",
    "\n",
    "    return lemmatized_col\n",
    "\n",
    "\n",
    "def lemmatize_text_spacy(doc: spacy.tokens.doc.Doc) -> str:\n",
    "\n",
    "    # Extract lemmatized words\n",
    "    # The word refunded does not get lemmatized, hence adding this special condition\n",
    "    lemmatized_words = (token.lemma_ for token in doc)\n",
    "\n",
    "    # Join the lemmatized words to form the lemmatized text\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = (\n",
    "    topic_df\n",
    "    .loc[lambda x: x['body'].notnull()]\n",
    "    .assign(cleaned_text=lambda x: lemmatize_pipe(\n",
    "        x['body'].str.lower() # lower case\n",
    "        .str.replace(r'https*.*? *|www\\..*? *|bit\\..*? *', \" \", \n",
    "                        regex=True) # Remove all url links\n",
    "        .apply(contractions.fix) # Apply Contractions function\n",
    "        .str.replace(r'[^a-z]', ' ', regex=True) # Remove all characters except alphabets\n",
    "        .str.replace('\\s+', \" \", regex=True) # Remove all extra spaces\n",
    "        .str.strip() # Remove leading and trailing spaces\n",
    "        .apply(remove_sw) # Remove stopwords\n",
    "    ))\n",
    "\n",
    "    # remove those rows with just empty text after preprocessing\n",
    "    .loc[lambda x:x['cleaned_text'].ne('')]\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "      <th>body_type</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1b0m6c1</td>\n",
       "      <td>r/CasualConversation is looking for new modera...</td>\n",
       "      <td>2024-02-26 16:48:14</td>\n",
       "      <td>Hello everyone,It's that time again, [we're lo...</td>\n",
       "      <td>post</td>\n",
       "      <td>22_subs_subreddits_trolls_subreddit</td>\n",
       "      <td>hello everyone time look new mod join team doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b5h6x7</td>\n",
       "      <td>Deleting social media was one of the best thin...</td>\n",
       "      <td>2024-03-03 13:29:12</td>\n",
       "      <td>I know that technically reddit is a social med...</td>\n",
       "      <td>post</td>\n",
       "      <td>1_fb_deleted_instagram_facebook</td>\n",
       "      <td>know technically reddit social medium not nega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1b5lu9j</td>\n",
       "      <td>Lonlieness is not about gender! My Opinion.</td>\n",
       "      <td>2024-03-03 16:56:09</td>\n",
       "      <td>Okay so I just wanted to get this off of my mi...</td>\n",
       "      <td>post</td>\n",
       "      <td>359_loneliness_lonely_epidemic_intiate</td>\n",
       "      <td>okay want get mind keep see people post man lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1b5jo0m</td>\n",
       "      <td>Have you gave up any hobbies?</td>\n",
       "      <td>2024-03-03 15:23:08</td>\n",
       "      <td>I used to be a doll collector. Then when I was...</td>\n",
       "      <td>post</td>\n",
       "      <td>-1_breakfast_cream_listening_bus</td>\n",
       "      <td>use doll collector tell weird old donate every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1b5c8ah</td>\n",
       "      <td>Is it me or has the world just stopped moving ...</td>\n",
       "      <td>2024-03-03 08:24:06</td>\n",
       "      <td>This is strange, but I feel like:1) I don‚Äôt re...</td>\n",
       "      <td>post</td>\n",
       "      <td>-1_breakfast_cream_listening_bus</td>\n",
       "      <td>strange feel like not remember anything happen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title  \\\n",
       "0  1b0m6c1  r/CasualConversation is looking for new modera...   \n",
       "1  1b5h6x7  Deleting social media was one of the best thin...   \n",
       "2  1b5lu9j       Lonlieness is not about gender! My Opinion.    \n",
       "3  1b5jo0m                      Have you gave up any hobbies?   \n",
       "4  1b5c8ah  Is it me or has the world just stopped moving ...   \n",
       "\n",
       "             timestamp                                               body  \\\n",
       "0  2024-02-26 16:48:14  Hello everyone,It's that time again, [we're lo...   \n",
       "1  2024-03-03 13:29:12  I know that technically reddit is a social med...   \n",
       "2  2024-03-03 16:56:09  Okay so I just wanted to get this off of my mi...   \n",
       "3  2024-03-03 15:23:08  I used to be a doll collector. Then when I was...   \n",
       "4  2024-03-03 08:24:06  This is strange, but I feel like:1) I don‚Äôt re...   \n",
       "\n",
       "  body_type                              topic_name  \\\n",
       "0      post     22_subs_subreddits_trolls_subreddit   \n",
       "1      post         1_fb_deleted_instagram_facebook   \n",
       "2      post  359_loneliness_lonely_epidemic_intiate   \n",
       "3      post        -1_breakfast_cream_listening_bus   \n",
       "4      post        -1_breakfast_cream_listening_bus   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  hello everyone time look new mod join team doc...  \n",
       "1  know technically reddit social medium not nega...  \n",
       "2  okay want get mind keep see people post man lo...  \n",
       "3  use doll collector tell weird old donate every...  \n",
       "4  strange feel like not remember anything happen...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "I know that technically reddit is a social media but for me it doesn't negatively effect my mental health, in fact it helps me quite a lot and I have gained quite a bit of knowledge from surfing it  Almost two months ago I deleted Instagram and Tiktok and honestly it was super difficult the first few days but..  \\- I had time to do things that I would otherwise have not done due to scrolling endlessly. I started working out and I am super proud of myself for that  \\- I don't really care about my looks as much anymore  \\- My screen time has decreased by quite a lot, I have social interactions in person way more now  So I guess what I'm saying is if you've been considering deleting social media for whatever reason, do it.\n",
      "\n",
      "Cleaned text:\n",
      "know technically reddit social medium not negatively effect mental health fact help quite lot gain quite bit knowledge surf almost two month ago delete instagram tiktok honestly super difficult first day time thing would otherwise not do due scroll endlessly start work super proud not really care look much anymore screen time decrease quite lot social interaction person way guess say consider delete social medium whatever reason\n"
     ]
    }
   ],
   "source": [
    "# Example of raw text vs clean text\n",
    "\n",
    "print(\"Raw text:\")\n",
    "print(cleaned_df.loc[1, 'body'])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Cleaned text:\")\n",
    "print(cleaned_df.loc[1, 'cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 131952 entries, 0 to 131951\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   post_id       131952 non-null  object\n",
      " 1   title         2285 non-null    object\n",
      " 2   timestamp     131952 non-null  object\n",
      " 3   body          131952 non-null  object\n",
      " 4   body_type     131952 non-null  object\n",
      " 5   topic_name    131952 non-null  object\n",
      " 6   cleaned_text  131952 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **[KeyBERT](\"https://maartengr.github.io/KeyBERT/api/keybert.html#:~:text=A%20minimal%20method%20for%20keyword,get%20a%20document%2Dlevel%20representation.\")** is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document.\n",
    "\n",
    "##### KeyBERT uses BERT embeddings and simple cosine similarity to find the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, cosine similarity is used to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Note**: Similarly to Topic modelling section, extracting keywords using keyBERT is a time consuming process. This task took around 2.5 hours to complete. Therefore the following codes will be commented out too. The dataframe containing the kewords has been saved to my local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "vectorizer = CountVectorizer(max_df=0.7, stop_words='english', ngram_range=(1, 3))\n",
    "\n",
    "\n",
    "model = KeyBERT(model=sentence_model)\n",
    "\n",
    "def extract_keywords(texts):\n",
    "\n",
    "    kw_list = model.extract_keywords(\n",
    "        texts,\n",
    "        vectorizer=vectorizer,\n",
    "        top_n=5,\n",
    "        use_mmr=True,\n",
    "        diversity=0.7\n",
    "    )\n",
    "\n",
    "    keywords=[\",\".join((k[0] for k in kw_tup)) for kw_tup in kw_list]\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d07e21059634ab8b3fb9ffc58597209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size=64\n",
    "nbr_of_batches=int(np.ceil(len(cleaned_df)/batch_size))\n",
    "\n",
    "batches = np.array_split(cleaned_df['cleaned_text'].to_list(), nbr_of_batches)\n",
    "\n",
    "keyword_list=[]\n",
    "\n",
    "for batch in tqdm(batches):\n",
    "    keywords_from_batch = extract_keywords(texts=batch)\n",
    "    keyword_list.extend(keywords_from_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df['keywords']=keyword_list\n",
    "cleaned_df.to_csv(\"datasets/reddit_posts_with_topics_keywords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "      <th>body_type</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1b0m6c1</td>\n",
       "      <td>r/CasualConversation is looking for new modera...</td>\n",
       "      <td>2024-02-26 16:48:14</td>\n",
       "      <td>Hello everyone,It's that time again, [we're lo...</td>\n",
       "      <td>post</td>\n",
       "      <td>22_subs_subreddits_trolls_subreddit</td>\n",
       "      <td>hello everyone time look new mod join team doc...</td>\n",
       "      <td>modmail courteous professional,grow strict quo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b5h6x7</td>\n",
       "      <td>Deleting social media was one of the best thin...</td>\n",
       "      <td>2024-03-03 13:29:12</td>\n",
       "      <td>I know that technically reddit is a social med...</td>\n",
       "      <td>post</td>\n",
       "      <td>1_fb_deleted_instagram_facebook</td>\n",
       "      <td>know technically reddit social medium not nega...</td>\n",
       "      <td>consider delete social,instagram tiktok,health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1b5lu9j</td>\n",
       "      <td>Lonlieness is not about gender! My Opinion.</td>\n",
       "      <td>2024-03-03 16:56:09</td>\n",
       "      <td>Okay so I just wanted to get this off of my mi...</td>\n",
       "      <td>post</td>\n",
       "      <td>359_loneliness_lonely_epidemic_intiate</td>\n",
       "      <td>okay want get mind keep see people post man lo...</td>\n",
       "      <td>man loneliness epidemic,understand gender thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1b5jo0m</td>\n",
       "      <td>Have you gave up any hobbies?</td>\n",
       "      <td>2024-03-03 15:23:08</td>\n",
       "      <td>I used to be a doll collector. Then when I was...</td>\n",
       "      <td>post</td>\n",
       "      <td>-1_breakfast_cream_listening_bus</td>\n",
       "      <td>use doll collector tell weird old donate every...</td>\n",
       "      <td>use doll collector,throw away hobby,day happy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1b5c8ah</td>\n",
       "      <td>Is it me or has the world just stopped moving ...</td>\n",
       "      <td>2024-03-03 08:24:06</td>\n",
       "      <td>This is strange, but I feel like:1) I don‚Äôt re...</td>\n",
       "      <td>post</td>\n",
       "      <td>-1_breakfast_cream_listening_bus</td>\n",
       "      <td>strange feel like not remember anything happen...</td>\n",
       "      <td>good year covid,like remember happen,people tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title  \\\n",
       "0  1b0m6c1  r/CasualConversation is looking for new modera...   \n",
       "1  1b5h6x7  Deleting social media was one of the best thin...   \n",
       "2  1b5lu9j       Lonlieness is not about gender! My Opinion.    \n",
       "3  1b5jo0m                      Have you gave up any hobbies?   \n",
       "4  1b5c8ah  Is it me or has the world just stopped moving ...   \n",
       "\n",
       "             timestamp                                               body  \\\n",
       "0  2024-02-26 16:48:14  Hello everyone,It's that time again, [we're lo...   \n",
       "1  2024-03-03 13:29:12  I know that technically reddit is a social med...   \n",
       "2  2024-03-03 16:56:09  Okay so I just wanted to get this off of my mi...   \n",
       "3  2024-03-03 15:23:08  I used to be a doll collector. Then when I was...   \n",
       "4  2024-03-03 08:24:06  This is strange, but I feel like:1) I don‚Äôt re...   \n",
       "\n",
       "  body_type                              topic_name  \\\n",
       "0      post     22_subs_subreddits_trolls_subreddit   \n",
       "1      post         1_fb_deleted_instagram_facebook   \n",
       "2      post  359_loneliness_lonely_epidemic_intiate   \n",
       "3      post        -1_breakfast_cream_listening_bus   \n",
       "4      post        -1_breakfast_cream_listening_bus   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  hello everyone time look new mod join team doc...   \n",
       "1  know technically reddit social medium not nega...   \n",
       "2  okay want get mind keep see people post man lo...   \n",
       "3  use doll collector tell weird old donate every...   \n",
       "4  strange feel like not remember anything happen...   \n",
       "\n",
       "                                            keywords  \n",
       "0  modmail courteous professional,grow strict quo...  \n",
       "1  consider delete social,instagram tiktok,health...  \n",
       "2  man loneliness epidemic,understand gender thin...  \n",
       "3  use doll collector,throw away hobby,day happy ...  \n",
       "4  good year covid,like remember happen,people tr...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_topic_kw = pd.read_csv(\"datasets/reddit_posts_with_topics_keywords.csv\")\n",
    "df_with_topic_kw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"0\":{\n",
      "        \"body\":\"Hello everyone,It's that time again, [we're looking for new mods to join our team!](https:\\/\\/docs.google.com\\/forms\\/d\\/e\\/1FAIpQLSeQNft07wRiSyPA5BvoFIJo9lpkx5WZhDsF5jnPCq9q7AL7HA\\/viewform)*Here are a few responsibilities you'll be faced with:** removing threads & comments that break da rules* keep the modqueue clear* help enforce our rules and etiquette* help maintain a healthy, nice and friendly atmosphere* handle modmails in a courteous and professional manner* hang out in our mod discord server* discuss things with the team* brainstorm new or current ideas* able to learn and grow with us* and more!---We do not have strict quotas but there is always something to do and we do expect our mods to be active in helping keep the queues clear.Do you think you have what it takes to be a moderator on one of the most friendly communities on Reddit? Give it a shot and apply!We are using a Google Form for our app (we **do not** collect your email address), fill out the application to the best of your ability.If you saw the link in our announcement bar and already applied, there's no need to apply again, you're all set! Note this may be a slow process for us, so hang in there.*Before you apply, please note:** If your account is under a year old or mostly empty, we likely won't consider the app---If you have any questions, concerns or compliments feel free to [send us a modmail.](https:\\/\\/www.reddit.com\\/message\\/compose?to=%2Fr%2FCasualConversation&subject=Mod Application Question)---[**CLICK HERE TO FILL OUT THE SUBREDDIT MOD APP**](https:\\/\\/docs.google.com\\/forms\\/d\\/e\\/1FAIpQLSeQNft07wRiSyPA5BvoFIJo9lpkx5WZhDsF5jnPCq9q7AL7HA\\/viewform)\",\n",
      "        \"keywords\":\"modmail courteous professional,grow strict quota,team brainstorm,link announcement bar,slow process hang\"\n",
      "    },\n",
      "    \"1\":{\n",
      "        \"body\":\"I know that technically reddit is a social media but for me it doesn't negatively effect my mental health, in fact it helps me quite a lot and I have gained quite a bit of knowledge from surfing it  Almost two months ago I deleted Instagram and Tiktok and honestly it was super difficult the first few days but..  \\\\- I had time to do things that I would otherwise have not done due to scrolling endlessly. I started working out and I am super proud of myself for that  \\\\- I don't really care about my looks as much anymore  \\\\- My screen time has decreased by quite a lot, I have social interactions in person way more now  So I guess what I'm saying is if you've been considering deleting social media for whatever reason, do it.\",\n",
      "        \"keywords\":\"consider delete social,instagram tiktok,health fact help,work super proud,surf month\"\n",
      "    },\n",
      "    \"2\":{\n",
      "        \"body\":\"Okay so I just wanted to get this off of my mind : I keep seeing how people post about the men's loneliness epidemic, and on top of that, I see so many men degrading women's experiences of loneliness. I believe Dr. K also made a video about this, and I found it incredibly important. Loneliness is about not having genuine connections with others. Sure, I have superficial connections with people at school, but in the end, they just make me feel more lonely. We should pay attention to the general loneliness caused by a lack of 'third places' and a lack of social interactions and understanding. It's not a gender thing. I've been lonely my whole life (I am female) and I had a hard childhood. It sucks seeing people on social media keep talking about men's loneliness while saying women have it so much easier... It's not about gender. The loneliness epidemic should be taken seriously.Edit: I also think loneliness is an individual thing. Everyone feels lonely for different reasons and comes from different backgrounds. It's not like all women experience the same childhood or anything similar. I understand that society expects men to be emotionless, and that's a specific issue for men. But that's all it is \\u2014 just one factor specific to men. Loneliness still affects everybody, and there are many other reasons.My main issues are that people keep saying how women have it easier, blame the \\\"male lonlieness\\\" on women and overlook women being lonely since \\\"women can't be lonely\\\". \",\n",
      "        \"keywords\":\"man loneliness epidemic,understand gender thing,different reason come,woman overlook,factor specific\"\n",
      "    },\n",
      "    \"3\":{\n",
      "        \"body\":\"I used to be a doll collector. Then when I was about 14 I told myself it was weird, I was too old, and I donated everything I had. I regret it to this day, although I am happy that I donated them rather than just throwing them away or whatever. What's a hobby you gave up? Was it for a silly reason like me, or a logical one?\",\n",
      "        \"keywords\":\"use doll collector,throw away hobby,day happy donate,tell weird old,silly reason like\"\n",
      "    },\n",
      "    \"4\":{\n",
      "        \"body\":\"This is strange, but I feel like:1) I don\\u2019t remember anything that happened in the years BEFORE Covid. All I know is that those were the good years (2010-2020)2) Since COVID, it seems like the world just STOPPED. Even with the reopen and stuff and people went out travelling, etc, but it just feels like we never \\u201cget past that point\\u201d. Nothing exciting, nothing fun, everything just seem normal, so so, daily life seems just meh, and we\\u2019re more disconnected and divisive than ever.\",\n",
      "        \"keywords\":\"good year covid,like remember happen,people travel feel,normal daily,meh disconnect divisive\"\n",
      "    },\n",
      "    \"5\":{\n",
      "        \"body\":\"Lately, I haven't been to a movie theater after the pandemic because I am afraid of breathing the air and I dislike movie theaters. Ever since the Batman incident in 2012 I stop going to movies because it is dangerous and a crazy person might show up. Also the movie tickets are now expensive with the popcorn, hot dogs, soda, and snacks.\",\n",
      "        \"keywords\":\"movie theater pandemic,breathing air dislike,batman,expensive popcorn hot,dog soda snack\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    (\n",
    "        df_with_topic_kw.loc[:5, ['body', 'keywords']]\n",
    "        .T\n",
    "        .to_json(indent=4)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the next notebook, we will analyze keyword frequncy for each of the topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
